%
% File project_checkpoint2.tex
%

\documentclass[11pt,letterpaper]{article}
\usepackage{naaclhlt2016}
\usepackage{times}
\usepackage{latexsym}

\naaclfinalcopy % Uncomment this line for the final submission

\title{Project Checkpoint 2 - Word Sense Disambiguation}

\author{Anthony Zheng, Yang Shi, Liang Zhang, Ruoyu Duan\\
      Word Sense Disambiguation\\
      EECS 595\\
	    University of Michigan\\
	    {\tt \{zhengant, gouzi, johzhang, duanry\}@umich.edu}
}

\date{}

\begin{document}

\maketitle

\section{Project Description}
Words with multiple meanings are everywhere in our daily lives. To be specific, the same word may have entirely different meanings in two different sentences or even in the same sentence. For example: 
\begin{enumerate}
  \item He sat down beside the Seine river \textbf{bank}.\\
  He deposited the money at the Chase \textbf{bank}\footnote{This example taken from https://github.com/Jeff09/Word-Sense-Disambiguation-using-Bidirectional-LSTM}.
  \item The family was hoping their \textbf{live} plants would \textbf{live}.
  \item After taking a shot with his \textbf{bow}, the archer took a \textbf{bow}. 
\end{enumerate}
In the first example, the first ``bank'' means \textit{land along the side of a river or lake}, but the second ``bank'' means \textit{a business that keeps and lends money and provides other financial services}. In the second example, the first ``live'' means \textit{animals or plants are alive, rather than being dead or artificial},  but the second ``live'' means \textit{someone lives in a particular place or with a particular person, their home is in that place or with that person}. In the third example, the first ``bow'' refers to the weapon for shooting arrows but the second ``bow'' is \textit{the act of bending all or part of the upper body}. While a human is generally able to determine the correct word sense in each case without too much effort, machines often have great difficulty with this task. In fact, some consider word sense disambiguation an AI-complete problem and many models do not even consider word sense and just treat all instances of a word the same.

In this project, we are aiming to use an unsupervised learning method to create a model that can infer when a word has multiple senses and categorize instances of the word into distinct senses. The idea is that one should be able to infer the sense based on the context. Thus, the model will first mine features from each word's context. Second, the model will run all of the feature vectors for a particular word through a clustering algorithm. Based on the results of the clustering algorithm, we can annotate the data to indicate word sense. Note that this does not necessarily map instances of a word to a particular dictionary definition - instead, it only identifies cases where the word sense differs and indicates them as such. We hope to try a variety of different algorithms and approaches that fit within our unsupervised scheme and compare their performances to see which one works best. The baseline we will compare against is the naive approach that just takes the most frequent word sense for each word, similar to what we did with part-of-speech tagging. 

Successful disambiguation of word senses can be extremely valuable to other natural language processing tasks. With a good model, we can preprocess any text data by annotating each word, then use this new text to perform our task of interest. Tasks that may care a lot about word sense include sentiment analysis, text summarization, question-answering systems, etc. Adding a word sense disambiguation model as a preprocessing step may improve the performance of the algorithms implemented for these related tasks. 

\section{Related Work}
Lee and Ng \shortcite{lee2002empirical} performed an empirical evaluation of knowledge sources and learning algorithms for word sense disambiguation. In this paper, several interesting preprocessing techniques are introduced like part-of-speech tagging of neighboring words, single words in the surrounding context, local collocation, syntactics relations, etc.; these are all steps we will consider doing for preprocessing and feature selection. Also, the author provides a variety of learning algorithms including Support Vector Machines, AdaBoost, Naive Bayes, Decision Trees and also some smoothing techniques - these are mainly supervised learning algorithms. In our project, we will be focusing mainly on unsupervised approaches but we may be able to adapt some of the methods described here so they also work without labeled data.

Yuan et. al. \shortcite{yuan2016semi}, explored an LSTM-based semi-supervised word sense disambiguation model, which uses LSTMs (long short-term memory) to perform word sense disambiguation. Since the LSTM-based supervised word sense disambiguation model is a very cutting-edge technique, we found that many researchers or teams have used this techniques a lot on the word sense disambiguation. We may explore adapting the algorithms described in this paper to work in an unsupervised manner.

Yarowsky \shortcite{yarowsky1995unsupervised} introduces two methods about how unsupervised word sense disambiguation rivals supervised one. One is called “One sense per collocation”, which uses the nearby words to build the feature vector for the input because they can provide strong and consistent clues to the sense of a target word. Another one is called “One sense per discourse”, which takes advantage of the regularity in conjunction with seperated models of local context. For the rest of the content, the author illustrated full steps of performing an unsupervised learning algorithm by the disambiguation of the given polysemous words. We can use this as a starting point and try to improve the algorithms described to get better performance.

\section{Data}
One dataset we may use is the ``word sense disambiguation corpera'' from Google AI. The corpera is used especially for training and testing model related to word sense disambiguation. The data is all in xml form. The data is divided into speech part and writing part and each parts contains large data from variety source, which can reduce data bias. Each file is filled with lots of sentences and ambiguous word is tagged with the reference of the position in dictionary. For the same word, by comparing the position in the dictionary, we can know whether the meaning of the word is different or not. While we will not be using the labels for developing the model, they can be useful for evaluating the model. 

We also found another helpful and similar dataset: the data of SemEval 2010 Word Sense Induction \& Disambiguation Task. The task for which this dataset was created called for disambiguation of just specific set of verbs and nouns. This may be more useful as a starting point since it is a simpler task than trying to jump right into annotating all word senses for all words. We may want to start with this dataset and if we are able to get a working model on it, start working with Google AI's corpus.

After analyzing the data, we found that it was interesting that for a lot of words with multiple meanings, the parts-of-speech among the different meanings of a word are different, like ``desert'' has three main meaning and the part of speech among them are ``noun'', ``verb'', ``adjective'', all are different. Another interesting example is ``live''. ``Live'' has two main meaning and the part of speech of the two meanings are ``verb'' and ``adjective'', which are also different. This tells us that a part-of-speech tag may be an important feature to include for our algorithm.

\section{Methodology}
We plan on taking an unsupervised approach to tackle word sense disambiguation, i.e. the input will be an arbitrary collection of texts. The model pipeline will look something like the following:
\begin{enumerate}
  \item Feature extraction on each word
  \item Clustering instances of each word to determine word sense(s)
  \item Annotating the data to disambiguate different word senses
\end{enumerate}

We describe each of these steps in greater detail in the following subsections, then we describe potential methods for evaluation.

\subsection{Feature Extraction}
The intuition here is that humans can determine word sense based on the context of the word, so we should design our features so that they somehow capture the information contained in the context. Furthermore, we want to eventually run the feature vectors mined for each word through a clustering algorithm so the features should somehow reflect similarity, i.e. feature vectors that are closer together according to some distance metric (e.g. Euclidean distance or cosine similarity) should be more similar in word sense than feature vectors that are further apart. Some sort of vector space model like Word2Vec (the continuous bag-of-words version since we want to generate features based on the context and not the word itself) makes sense as a starting point. 

We may also include features like a part-of-speech tag since different part-of-speech tags can be a good indicator of different word senses. We will want to think about how to best add it to the feature vector so it does not adversely affect the clustering since we still want similar word senses to generate similar feature vectors. This might be as simple as having ``similar'' parts of speech like noun and proper noun are encoded as numbers that are close to each other.

\subsection{Clustering}
To disambiguate the various word senses for a particular word, we can extract the features of each instance of the word and cluster the generated feature vectors. Hopefully, the different word senses will create distinct clusters in the data and each cluster will be assigned a different word sense. Note that this by itself will not provide insight into what the word senses are, only that the word senses are different. 

Because we do not know how many different word senses a word might have in advance, we must pick our clustering method in a way so that it can pick the number of clusters on its own. To do this, we can either use a clustering algorithm that does not require us to specify a number of clusters at all (e.g., DBSCAN\footnote{https://www.aaai.org/Papers/KDD/1996/KDD96-037.pdf}) or we can pick a standard clustering algorithm like $k$-means, restrict the number of clusters to a particular reasonable range ($[1,n]$, where $n$ is the maximum number of word senses that we believe a word will have), and picking the value that optimizes some performance measure like the Bayesian Information Criterion (BIC)\footnote{https://www.jstor.org/stable/2958889}. The plan is to try multiple approaches and compare their performances.

\subsection{Annotation}
Once the instances of each word have been clustered, we can assign word senses to each cluster, then annotate the input data in a way so that instances with different word senses are considered separate. This can be as simple as assigning a number to each word sense and appending the corresponding number to the end of each word, e.g. ``apple'' (the fruit) could become ``apple0'' and ``Apple'' (the company) could become ``Apple1''. As mentioned earlier, the methodology here would not infer the meanings ``fruit'' and ``company'' themselves but instead just determine that these two word senses are distinct from each other.

\subsection{Evaluation}
There are a multiple approaches we can take for evaluation. First, we can take a dataset labeled for supervised word sense disambiguation and use them to evaluate the performance of the clustering algorithm using standard measures like V-measure\footnote{http://www.aclweb.org/anthology/D07-1043}. Schutze~\shortcite{schutze92context} also suggests a way to do this without labeled data where one artificially creates ambiguities by merging pairs of words and testing the model's ability to determine what the original word was. For example, all instances of ``train'' and ``tennis'' could be replaced with ``traintennis'' and the model is tasked with determine which instances of ``traintennis'' were originally ``train'' and which ones were originallly ``tennis''. We can generalize this method by merging an arbitrary number of words. 

We can also view our model as a preprocessing step to a larger task - this means that we can evaluate our model by seeing if annotating the data with word senses first improves performance on some other task.  

\bibliography{project_checkpoint2}
\bibliographystyle{naaclhlt2016}


\end{document}